{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIROH Developers Conference: Hydrological Applications of ML\n",
    "## CNNs for Predicting Daily Orographic Precipitation Gradients for Atmospheric Downscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this exercise, we'll build a convolutional neural network (CNN) for regression of orographic precipitation gradients of Northern Utah. Other uses include:\n",
    "\n",
    "- Image classification\n",
    "- Image denoising or reconstruction\n",
    "- \n",
    "\n",
    "But all methods are based around two basic operations: \n",
    "\n",
    "- Convolution: in this step, the network learns a series of kernels or filters that transform the original image in some way. These are similar to filters that are used in standard image processing (e.g. low-pass filters), but filters are chosen by how well the transformed image maps to the outcome variable. To put this another way, these filters identify shapes or features that are important in differentiating between different outcomes\n",
    "- Max-pooling: in this step, the image resolution is transformed. In general, the resolution is halved, by aggregating groups of four pixels in a two by two window. \n",
    "\n",
    "In general, these steps are repeated several times. As this progresses, the small shapes identified in the first set of convolutions are progressively combined into larger structures. For example, a series of small curves or lines could be aggregated into a cat's eye.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPG Regression\n",
    "\n",
    "### Data processing\n",
    "Let's start, as usual, by loading the libraries we'll need for the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import nclcmaps as ncm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the path to the data folders and download the datasets. If you have any questions about setting this path, please ask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the atmospheric data\n",
    "path      = \"../datasets/era5_atmos/\"\n",
    "IVT       = xr.open_dataset(f\"{path}IVT_sfc.nc\")\n",
    "precip    = xr.open_dataset(f\"{path}precip_sfc.nc\")*1000\n",
    "temp700   = xr.open_dataset(f\"{path}temp_700.nc\")-273.15\n",
    "uwinds700 = xr.open_dataset(f\"{path}uwnd_700.nc\")\n",
    "vwinds700 = xr.open_dataset(f\"{path}vwnd_700.nc\")\n",
    "hgt500    = xr.open_dataset(f\"{path}hgt_500.nc\")/9.81\n",
    "\n",
    "# load in the OPGs\n",
    "path = \"../datasets/facets_and_opgs/\"\n",
    "opg  = pd.read_csv(f\"{path}winter_northernUT_opg.csv\", index_col=0)\n",
    "facet_list = opg.columns.values\n",
    "opg = opg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the datasets - Since the values between the atmospheric variables contain a large variability in scale, we standardize each variable individually. This levels the playing field, so to speak, so that all atmospheric variables are taken into account. This also applies to the facet OPGs, as a machine learning model has a better time predicting values between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IVT       = (IVT - IVT.mean(dim=\"time\")) / IVT.std(dim=\"time\")\n",
    "precip    = (precip - precip.mean(dim=\"time\")) / precip.std(dim=\"time\")\n",
    "temp700   = (temp700 - temp700.mean(dim=\"time\")) / temp700.std(dim=\"time\")\n",
    "uwinds700 = (uwinds700 - uwinds700.mean(dim=\"time\")) / uwinds700.std(dim=\"time\")\n",
    "vwinds700 = (vwinds700 - vwinds700.mean(dim=\"time\")) / vwinds700.std(dim=\"time\")\n",
    "hgt500    = (hgt500 - hgt500.mean(dim=\"time\")) / hgt500.std(dim=\"time\")\n",
    "\n",
    "opg_mean = np.nanmean(opg, axis=0) # Save OPG mean and standard deviation so that we can\n",
    "opg_std  = np.nanstd(opg, axis=0)  # convert back to true values from standardized values later on.\n",
    "opg = (opg - opg_mean) / opg_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, when there lacks an OPG observation at a facet, the value was set to NaN. However, machine learning algorithms are unable to predict NaNs, so days without observations are set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opg[np.isnan(opg)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To input the atmospheric data into the CNN model, the atmospheric variables are combined into one array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 19, 27, 6)\n"
     ]
    }
   ],
   "source": [
    "# Combine the atmospheric data into one array\n",
    "atmos = np.concatenate((IVT.IVT.values[...,np.newaxis], \n",
    "                        precip.precip.values[...,np.newaxis],\n",
    "                        temp700.temp.values[...,np.newaxis],\n",
    "                        uwinds700.uwnd.values[...,np.newaxis],\n",
    "                        vwinds700.vwnd.values[...,np.newaxis],\n",
    "                        hgt500.hgt.values[...,np.newaxis]), axis=3)\n",
    "print(np.shape(atmos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output lists the dimensions of all the atmospheric variables combined. It contains 2708 observation days or 'images', with a 19 x 27 grid of latitudes and longitudes. Each atmospheric variable represents a channel, here we have 6 channels. \n",
    "\n",
    "Channels can be viewed as layers of the image. For example, when using visual images of RGB colors, there would be 3 channels. \n",
    "\n",
    "CNNs are not limited only to images of shape `(samples, height, width, channels)`, CNNs can be used to evaluate vector data `(samples, features)`, timeseries data `(samples, timesteps, features)`, and video data `(samples, frames, height, width, channels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude  = 19 # height\n",
    "longitude = 27 # width\n",
    "channels  = 6  # channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a ML model, the datasets are split into training, validation, and testing subsets. The model is trained on the training subset, evaluated on the validation subset, and tested on the testing subset. \n",
    "\n",
    "Often, the datasets are randomly shuffled to produce training, validation, and testing subsets. Or the overlap of data can be important and the data is randomly chosen by week, months, or years.\n",
    "\n",
    "The split of these datasets are user defined, often being a 70/15/15% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rand_ind = np.random.permutation(np.arange(np.shape(atmos)[0]))\n",
    "\n",
    "train_atmos = atmos[rand_ind[:1896], ...]\n",
    "test_atmos  = atmos[rand_ind[1896:2302], ...]\n",
    "val_atmos   = atmos[rand_ind[2302:], ...]\n",
    "\n",
    "train_opg = opg[rand_ind[:1896], ...]\n",
    "test_opg  = opg[rand_ind[1896:2302], ...]\n",
    "val_opg   = opg[rand_ind[2302:], ...]\n",
    "\n",
    "print(np.shape(test_atmos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last parameter we'll set here is the batch size. This is the same parameter that we have used before to control the rate at which the network weights are updated. We'll also use this to control the number of images that are loaded into memory at any step. This is very useful if you're working on a computer with limited memory (like my old laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has several functions to facilitate working with images. We'll start by creating an image *generator*. This acts a bit like a pipeline and will carry various pre-processing steps. These include data augmentation: simple transformations of the images to supplement the original image. We're not going to use that here, but some example code is given in the appendix to illustrate how you might use this.\n",
    "\n",
    "We'll create a generator for the training images. This will rescale each channel to a 0-1 range (the RGB channels have values between 0 and 255), and it will hold aside 30% of the training images for validation. We'll use this to check for overfitting during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function we'll use is a *flow* function. This function controls how Keras will read in the images for any training step (i.e. any update of the network weights). There are several arguments here:\n",
    "\n",
    "- `train_image_files_path`: The path to the top-level folder containing the training images\n",
    "- `train_data_gen`: The image data generator\n",
    "- `subset`: The subset of images to use from the generator for training. As we set the `validation_size` to 0.3, this will be 1 - 0.3 = 0.7, or 70% of the images\n",
    "- `target_size`: The size for rescaling each image\n",
    "- `class_mode`: The type of label used (this will one-hot encode the labels of the images)\n",
    "- `classes`: The set of categories to use. This is the list we defined earlier and needs to match the subfolder names. If this is not included, this will use all subfolders, and create a list of labels\n",
    "- `batch_size`: The number of images to import for any update step\n",
    "- `seed`: a value to initialize the random number generator (this is only there to ensure consistent results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will tell you how many images (and classes) it found in the folder you defined. If this is 0, go back and check the folder path you defined earlier. We'll also create a flow for the validation images. The only difference here is in the definition of the subset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these flow generators contain various useful bits of information. For example, to check the number of images (we'll also use this number when training the model):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can get the number of images per class (type of fruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which suggest this is a relatively well-balanced dataset. This generator also contains various information about the files, resolution, channels, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "Let's now set up the model. As this is quite a complex model, we'll do this as a series of steps rather than in one go. First, import all the modules from Keras that we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (InputLayer, Conv2D,\n",
    "                          Dense,\n",
    "                          ReLU,LeakyReLU,\n",
    "                          BatchNormalization, \n",
    "                          MaxPooling2D, \n",
    "                          Dropout,\n",
    "                          Flatten)\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a template sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the first hidden layer. This is a convolutional layer, where we'll create 16 filters (or convolutions) based on the original images, with a 3x3 kernel. We'll pad the output of this layer so that it has the same size as the input (`same`). Note that we also need to define the size of the input tensors (width, height and channels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(InputLayer(shape=(latitude, longitude, channels)))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3), padding = 'same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then take the output of this layer and pass it through a ReLU activation function (this could have been included directly in the convolutional layer, but this allows a little more control on the process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add a max-pooling layer. As a reminder, this reduces the resolution of the output from the previous layer by a simple filter, forcing the next layer of the network to focus on larger image features. We'll also add a dropout layer. This is a form of regularization. It randomly sets some connection weights to 0 (i.e. having no contribution to the model), which can reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another convolutional layer, this time with 32 filters, and pass this through a different activation function (a leaky ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'same'))\n",
    "model.add(LeakyReLU(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the output of this function and normalize the weights. This is a simple method that adjusts the mean weight to close to zero and reduces the amount variation. This helps avoid gradient problems with very small or very large weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll run the output of this through a max-pooling function with dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add layers to connect the output of this last max-pooling step to the output (the fruit classes). The first thing we need to do is to flatten the output. The output of the max-pooling is a tensor of shape (5, 5, 32). The size of 5 is a result of the two max-pooling operations and the 32 is the number of filters from the second convolution. The `Flatten()` function will flatten this into a rank 1 tensor of shape (800). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll pass this flattened layer through a dense layer, with a ReLU activation and a dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 100))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout(.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to output predictions. As this is a multiclass task, the final layer needs to have the same number of nodes as classes (16). This is passed through a softmax activation function. This transforms the predictions for all classes into probabilities (i.e. they have to sum to 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = len(facet_list), activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More practically, we'll create a function that will build the model in one go. This will allow us to easily create new versions for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential([  \n",
    "        InputLayer(shape=(latitude, longitude, channels)),\n",
    "        Conv2D(filters = 16, kernel_size = (3,3), padding = 'same', activation = 'relu'),\n",
    "        MaxPooling2D(pool_size = (2,2)),\n",
    "        Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size = (2,2)),\n",
    "        Dropout(.25),\n",
    "        Flatten(),\n",
    "        Dense(units = 100, activation = 'relu'),\n",
    "        Dense(units = 100, activation = 'relu'),\n",
    "        Dropout(.25),\n",
    "        Dense(units = len(facet_list))\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the whole thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has a little under 87,000 parameters or weights to train (hence the need for a lot of images). Note that there are small set of non-trainable parameter from the normalization layer. \n",
    "\n",
    "We can also generate a simple visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compile the model. This is an optimization function, a loss function and performance metric. A good option for the loss function in a regression problem is `mean_squared_error`, metrics allows the user to add aditional functions to monitor training. The optimizer... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error', \n",
    "              metrics = [\"mean_absolute_error\"], \n",
    "              optimizer = RMSprop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We'll now train the model for 20 epochs using the `fit()` method upon our built model. We specify:\n",
    "\n",
    "- `train_atmos`: The training subset of the atmospheric variables\n",
    "- `train_opg`: The training subset of the observed OPGs\n",
    "- `batch_size`: The number of samples or observation days used per gradient update.\n",
    "- `epochs`: The number of full training iterations.\n",
    "- `validation_data`: The validation subset of the atmospheric variables and observed OPGs\n",
    "\n",
    "This takes a few minutes to train (on my laptop). It's worth remembering what is going on here: the algorithm is reading in batches of 32 images, rescaling them, updating model weights through back propagation and then repeating the whole thing 50 times. As we previously defined a separate validation subset, this routine will calculate two losses:\n",
    "- The training loss. This is how accurately the model can predict OPG from the atmospheric 'images' that are being used to update the weights\n",
    "- The validation loss. This is how accurately the model can predict OPG from a set of atmospheric 'images' that are not used in updating the weights\n",
    "\n",
    "As the model continues to train, you should see the loss decrease for both of these, but will likely stabilize at a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "hist = model.fit(train_atmos, train_opg, # training data\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,                        # epochs\n",
    "                  validation_data = (val_atmos, val_opg), # validation data\n",
    "                  verbose = 1)                           # print progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the evolution of the loss function and the actual versus predicted OPGs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(hist_obs):\n",
    "    plt.plot(hist_obs.history['loss'], label='train')\n",
    "    plt.plot(hist_obs.history['val_loss'], label='valid')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_act_pred():\n",
    "    # Using our fitted model, predict OPGs from the testing subset.\n",
    "    predicted = model.predict(test_atmos)\n",
    "    \n",
    "    # Convert from standardized OPG to mm/m OPG\n",
    "    actual    = (test_opg * opg_std) + opg_mean\n",
    "    predicted = (predicted * opg_std) + opg_mean\n",
    "    \n",
    "    # Since days without OPG observations are set to zero, lets reshape our OPGs and remove zero OPG days\n",
    "    actual    = np.reshape(actual, -1)\n",
    "    predicted = np.reshape(predicted, -1)\n",
    "    idx       = actual != 0.0 # Identify days with OPG observations\n",
    "    actual    = actual[idx]\n",
    "    predicted = predicted[idx]\n",
    "    \n",
    "    # Formulate the x and y axis limits\n",
    "    max_val = np.max((np.max(predicted[:]), np.max(actual[:])))\n",
    "    min_val = np.min((np.min(predicted[:]), np.min(actual[:])))\n",
    "    max_val = 0.05\n",
    "    min_val = -0.05\n",
    "    \n",
    "    # Define plot colormap\n",
    "    cmap_gnuplot2 = ncm.cmap(\"MPL_gnuplot2\")\n",
    "    \n",
    "    # Define label sizes\n",
    "    label_size = 16\n",
    "    tick_size = 14\n",
    "    \n",
    "    # Create Figure\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    \n",
    "    # Formulate the heatmap variables\n",
    "    heatmap, xedges, yedges = np.histogram2d(np.reshape(actual, -1), np.reshape(predicted, -1), bins=100,\n",
    "                                 range=[[min_val, max_val],[min_val, max_val]])\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap_gnuplot2, norm=colors.LogNorm())\n",
    "    \n",
    "    # Add labels, gridlines, and colorbar\n",
    "    plt.xlabel(\"Actual\", fontsize=label_size)\n",
    "    plt.ylabel(\"Predicted\", fontsize=label_size)\n",
    "    plt.grid(True)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "plot_loss(hist)\n",
    "plot_act_pred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows a steep decline in both loss values, but no real improvement beyond epoch 20. It's quite likely that this model has overfit - become too tuned to the training data to allow prediction. There are a variety of ways we can avoid this, but a simple one is to slow the rate at which the model learns. We'll refit the model, but will add a `learning_rate` parameter to the compilation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', \n",
    "              metrics = [\"mean_absolute_error\"], \n",
    "              optimizer = RMSprop(learning_rate=1e-4))\n",
    "epochs = 80\n",
    "hist = model.fit(train_atmos, train_opg, # training data\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,                        # epochs\n",
    "                  validation_data = (val_atmos, val_opg), # validation data\n",
    "                  verbose = 1)                           # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist)\n",
    "plot_act_pred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot now shows a good evolution of the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Along with the loss function, the structure of the CNN can be changed. I suggest seeing what happens when you do the following:\n",
    "- `Change the Dense layers to have units=10`\n",
    "- `Uncomment the additional MaxPooling2D and Conv2D layers`\n",
    "- `Increase Conv2D filters to 64 and 128, respectively`\n",
    "- `Change the loss function to mean_absolute_error`\n",
    "- `Change the Dense layers to have activation=\"sigmoid\"`\n",
    "\n",
    "Pay mind to how changing these parameters affect the number of parameters to train, how long training time becomes, and the 2D histogram of actual versus predicted OPGs. Feel free to try your own edits to inspect how it affects the testing outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([  \n",
    "    InputLayer(shape=(latitude, longitude, channels)),\n",
    "    Conv2D(filters = 16, kernel_size = (3,3), padding = 'same', activation = 'sigmoid'), # <-- Change the filter # of the Convolutional Layers\n",
    "    MaxPooling2D(pool_size = (2,2)),\n",
    "    Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'sigmoid'), # <-- Change the filter # of the Convolutional Layers\n",
    "    # MaxPooling2D(pool_size = (2,2)),\n",
    "    # Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size = (2,2)),\n",
    "    Dropout(.25),\n",
    "    Flatten(),\n",
    "    Dense(units = 100, activation = 'sigmoid'),  # <-- Change the units of the Dense Layers\n",
    "    Dense(units = 100, activation = 'sigmoid'),  # <-- Change the units of the Dense Layers\n",
    "    Dropout(.25),\n",
    "    Dense(units = len(facet_list), activation = 'linear') ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'mean_squared_error',          # <-- Change Loss Function Here\n",
    "              metrics = [\"mean_absolute_error\", \"mean_squared_error\"], \n",
    "              optimizer = RMSprop(learning_rate=1e-4))\n",
    "\n",
    "epochs = 80\n",
    "hist = model.fit(train_atmos, train_opg, \n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,                      \n",
    "                  validation_data = (val_atmos, val_opg),\n",
    "                  verbose = 0)                     # <-- If you want to silence training, set to 0, if you want to watch it train, set to 1\n",
    "\n",
    "plot_loss(hist)\n",
    "plot_act_pred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the differences between outputs when changing the hyperparameters:\n",
    "- `Change the Dense layers to have units=10` - The mean squared error flatlines and the predicted versus actual look terrible.\n",
    "- `Uncomment the additional MaxPooling2D and Conv2D layers` - Additional filters did not better the\n",
    "- `Increase Conv2D filters to 64 and 128 respectively` - Additional filters did not better the\n",
    "- `Change the loss function to mean_absolute_error` - The shape of the predicted versus actual changes.\n",
    "\n",
    "What you choose for hyperparameters and structure all depends on what your goals are. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our fitted model, predict OPGs from the testing subset.\n",
    "predicted = model.predict(test_atmos)\n",
    "\n",
    "# Convert from standardized OPG to mm/m OPG\n",
    "actual    = (test_opg * opg_std) + opg_mean\n",
    "predicted = (predicted * opg_std) + opg_mean\n",
    "\n",
    "# Since days without OPG observations are set to zero, lets reshape our OPGs and remove zero OPG days\n",
    "actual    = np.reshape(actual, -1)\n",
    "predicted = np.reshape(predicted, -1)\n",
    "idx       = actual != 0.0 # Identify days with OPG observations\n",
    "actual    = actual[idx]\n",
    "predicted = predicted[idx]\n",
    "\n",
    "# Formulate the x and y axis limits\n",
    "max_val = np.max((np.max(predicted[:]), np.max(actual[:])))\n",
    "min_val = np.min((np.min(predicted[:]), np.min(actual[:])))\n",
    "max_val = 0.05\n",
    "min_val = -0.05\n",
    "\n",
    "# Define plot colormap\n",
    "cmap_gnuplot2 = ncm.cmap(\"MPL_gnuplot2\")\n",
    "\n",
    "# Define label sizes\n",
    "label_size = 16\n",
    "tick_size = 14\n",
    "\n",
    "# Create Figure\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Formulate the heatmap variables\n",
    "heatmap, xedges, yedges = np.histogram2d(np.reshape(actual, -1), np.reshape(predicted, -1), bins=100,\n",
    "                             range=[[min_val, max_val],[min_val, max_val]])\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap_gnuplot2, norm=colors.LogNorm())\n",
    "\n",
    "# Add labels, gridlines, and colorbar\n",
    "plt.xlabel(\"Actual\", fontsize=label_size)\n",
    "plt.ylabel(\"Predicted\", fontsize=label_size)\n",
    "plt.grid(True)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
