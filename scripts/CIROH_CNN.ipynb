{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIROH Developers Conference: Hydrological Applications of ML\n",
    "### CNNs for Predicting Daily Orographic Precipitation Gradients (OPGs) for Atmospheric Downscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise, we'll build a convolutional neural network (CNN) for regression of orographic precipitation gradients of Northern Utah. Other uses include:\n",
    "\n",
    "- Image classification\n",
    "- Image denoising or reconstruction\n",
    "\n",
    "But all methods are based around two basic operations: \n",
    "\n",
    "- Convolution: in this step, the network learns a series of kernels or filters that transform the original image in some way. These are similar to filters that are used in standard image processing (e.g. low-pass filters), but filters are chosen by how well the transformed image maps to the outcome variable. To put this another way, these filters identify shapes or features that are important in differentiating between different outcomes\n",
    "- Max-pooling: in this step, the image resolution is transformed. In general, the resolution is halved, by aggregating groups of four pixels in a two by two window. \n",
    "\n",
    "In general, these steps are repeated several times. As this progresses, the small shapes identified in the first set of convolutions are progressively combined into larger structures. For example, a series of small curves or lines could be aggregated into a cat's eye.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "##### Let's start, as usual, by loading the libraries we'll need for the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "# Here, Pandas will be used to load in csv datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Atmospheric data are often put in NetCDF data \n",
    "# files, which are best opened using Xarray\n",
    "import xarray as xr\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# You can also load in other python scripts that contain \n",
    "# functions like a Python Library. This one contains a list \n",
    "# of Colormaps for plotting\n",
    "import nclcmaps as ncm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we set the path to the data folders and download the datasets. If you have any questions about setting this path, please ask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the atmospheric data\n",
    "path      = \"../datasets/era5_atmos/\"\n",
    "IVT       = xr.open_dataset(f\"{path}IVT_sfc.nc\")\n",
    "precip    = xr.open_dataset(f\"{path}precip_sfc.nc\")*1000\n",
    "temp700   = xr.open_dataset(f\"{path}temp_700.nc\")-273.15\n",
    "uwinds700 = xr.open_dataset(f\"{path}uwnd_700.nc\")\n",
    "vwinds700 = xr.open_dataset(f\"{path}vwnd_700.nc\")\n",
    "hgt500    = xr.open_dataset(f\"{path}hgt_500.nc\")/9.81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The atmospheric data are in NetCDF files. This file type is used because it includes information about the dataset it contains. Below, lets print what is inside of the precip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above we first see the `Dimentions` of the dataset: Latitude, Longitude, and time. These dimention variables are listed under `Coordinates`, which allows us to easily work with the precip data. Then `Data variables` lists precip. A NetCDF file has the ability to list numerous `Data variables` of any combination of the above `Coordinates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the OPGs\n",
    "path = \"../datasets/facets_and_opgs/\"\n",
    "opg  = pd.read_csv(f\"{path}winter_northernUT_opg.csv\", index_col=0)\n",
    "print(opg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above shows the opg file when opened using the Pandas library. It contains an index of winter season dates from 1988 to 2017 and the column headers are each facet's identification number. Within the file are the observed OPGs. For days without precipitation or an OPG obsservation due to mesonet station counts, the value is set to NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing the Datasets:\n",
    "\n",
    "##### Since the values between the atmospheric variables contain a large variability in scale, we standardize each variable individually. This levels the playing field, so to speak, so that all atmospheric variables are taken into account within the CNN. This also applies to the facet OPGs, as a machine learning model is better equipted predicting values between -1 and 1.\n",
    "\n",
    "##### In other cases, you may prefer to normalize the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the atmospheric data\n",
    "\n",
    "# With Xarray datasets, operations are available to be applied along \n",
    "# existing dimensions by name. Here we are able to formulate the \n",
    "# mean and standard deviation along the time dimension. If you're \n",
    "# curious to see what other function operations that can be applied, \n",
    "# go to this link: https://docs.xarray.dev/en/stable/api.html\n",
    "IVT       = (IVT - IVT.mean(dim=\"time\")) / IVT.std(dim=\"time\")\n",
    "precip    = (precip - precip.mean(dim=\"time\")) / precip.std(dim=\"time\")\n",
    "temp700   = (temp700 - temp700.mean(dim=\"time\")) / temp700.std(dim=\"time\")\n",
    "uwinds700 = (uwinds700 - uwinds700.mean(dim=\"time\")) / uwinds700.std(dim=\"time\")\n",
    "vwinds700 = (vwinds700 - vwinds700.mean(dim=\"time\")) / vwinds700.std(dim=\"time\")\n",
    "hgt500    = (hgt500 - hgt500.mean(dim=\"time\")) / hgt500.std(dim=\"time\")\n",
    "\n",
    "# extract the facet list and the OPG values\n",
    "facet_list = opg.columns.values\n",
    "opg = opg.values\n",
    "\n",
    "# Standardize the OPGs\n",
    "# We are saving the OPG mean and standard deviation so that \n",
    "# we can convert the predicted standardized values back to the \n",
    "# true OPG values later on\n",
    "opg_mean = np.nanmean(opg, axis=0)\n",
    "opg_std  = np.nanstd(opg, axis=0)\n",
    "opg = (opg - opg_mean) / opg_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unique to the OPG dataset, for days without precipitation, the value within the array are NaNs. However, machine learning algoirthms of this architecture are unable to predict NaN as an output. Therefore, days without observations are set to zero, as it is the average value of the dataset.\n",
    "\n",
    "##### In classification tasks, the missing values are often set to a new catagory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opg[np.isnan(opg)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To input the atmospheric data into the CNN model, the atmospheric variables are combined into one array. Additionally, when working with Tensorflow, all datasets need to be either NumPy or Tensorflow Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the atmospheric data into one array\n",
    "atmos = np.concatenate((IVT.IVT.values[...,np.newaxis], \n",
    "                        precip.precip.values[...,np.newaxis],\n",
    "                        temp700.temp.values[...,np.newaxis],\n",
    "                        uwinds700.uwnd.values[...,np.newaxis],\n",
    "                        vwinds700.vwnd.values[...,np.newaxis],\n",
    "                        hgt500.hgt.values[...,np.newaxis]), axis=3)\n",
    "print(np.shape(atmos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above output lists the dimensions of all the atmospheric variables combined. It contains 2708 observation days or 'images', with a 19 x 27 grid of latitudes and longitudes. Each atmospheric variable represents a channel, here we have 6 channels. \n",
    "\n",
    "##### Channels can be viewed as layers of the image. For example, when using visual images of RGB colors, there would be 3 channels. \n",
    "\n",
    "##### CNNs are not limited only to images of shape `(samples, height, width, channels)`, CNNs can be used to evaluate vector data `(samples, features)`, timeseries data `(samples, timesteps, features)`, and video data `(samples, frames, height, width, channels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude  = 19 # height\n",
    "longitude = 27 # width\n",
    "channels  = 6  # channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When creating a ML model, the datasets are split into training, validation, and testing subsets. The model is trained on the training subset, evaluated on the validation subset, and tested on the testing subset. \n",
    "\n",
    "##### These datasets are often randomly shuffled to produce training, validation, and testing subsets. If redundency exists in your dataset, or in our case, multipday precipitation events, it can be important to split the subsets differently. Such as by week, months, or years. \n",
    "\n",
    "##### Here, we'll split these datasets at random using a 70/15/15% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rand_ind = np.random.permutation(np.arange(np.shape(atmos)[0]))\n",
    "\n",
    "train_atmos = atmos[rand_ind[:1896], ...]\n",
    "test_atmos  = atmos[rand_ind[1896:2302], ...]\n",
    "val_atmos   = atmos[rand_ind[2302:], ...]\n",
    "\n",
    "train_opg = opg[rand_ind[:1896], ...]\n",
    "test_opg  = opg[rand_ind[1896:2302], ...]\n",
    "val_opg   = opg[rand_ind[2302:], ...]\n",
    "\n",
    "print(np.shape(test_atmos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The last parameter we'll set here is the batch size. This parameter controls how many samples are processed in training simultaneously, typically between 8 and 128. The dataset is separated into batches, processed by the model, and the weights within the model are adjusted. Once all batches are processed, one epoch has been completed.\n",
    "\n",
    "##### This also controls how much memory is used per step of training, which is very useful if you're training on a computer with limited memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN Model\n",
    "\n",
    "##### Let's now set up the model. As this is quite a complex model, we'll do this as a series of steps rather than in one go. First, import all the modules from Tensorflow Keras that we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load in a function to allow us to Sequentially build the CNN architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Here we load in the CNN layers\n",
    "from tensorflow.keras.layers import (InputLayer, Conv2D, Dense, ReLU, BatchNormalization, \n",
    "                                     MaxPooling2D, Dropout, Flatten)\n",
    "# Here we load in the optimizer we will use\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, create a template sequential model, this way we can build the model incrementally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, we'll use `InputLayer` to declare the shape of the input data. Notice that we care about the entire input set, not the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(InputLayer(shape=(latitude, longitude, channels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we add the first hidden layer, `Conv2D`. This is a convolutional layer, where we'll create 16 filters (or convolutions) based on the original images, with a 3x3 kernel. We'll add padding to the input of the `Conv2D` layer, this adds the appropriate number of rows and columns around the input image so that the output image is the `same` height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 16, kernel_size = (3,3), padding = 'same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll then take the output of this layer and pass it through a `ReLU()` activation function (this could have been included directly in the convolutional layer, but this allows a little more control on the process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we add a `MaxPooling2D` layer. As a reminder, this reduces the resolution of the output from the previous layer by a simple filter, forcing the next layer of the network to focus on larger image features. We'll also add a `Dropout` layer. This is a form of regularization. It randomly sets some connection weights to 0 (i.e. having no contribution to the model), which can reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's add another convolutional layer, this time with 32 filters, and pass this through a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'same'))\n",
    "model.add(ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll take the output of this function and normalize the weights. This is a simple method that adjusts the mean weight to close to zero and reduces the amount of variation. This helps avoid gradient problems with very small or very large weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And we'll run the output of this through a max-pooling function with dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we'll add layers to connect the output of this last `MaxPooling2D` step to the output (the OPGs). The first thing we need to do is to `Flatten()` the output. The output of the max-pooling is a tensor of shape (4, 6, 32). The size of 4x5 is a result of the two max-pooling operations and the 32 is the number of filters from the second convolution. The `Flatten()` function will flatten this into a rank 1 tensor of shape (768). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we'll pass this flattened layer through a `Dense()` layer, which are densely conncted neural layers. And then through `ReLU()` activation and a dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 100))\n",
    "model.add(ReLU())\n",
    "\n",
    "model.add(Dense(units = 100))\n",
    "model.add(ReLU())\n",
    "\n",
    "model.add(Dropout(.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, we need to output predictions. As this is a regression task, the final layer needs to have the same number of nodes as northern Utah facets, and the output is passed through a linear activation function. Linear activation functions are typically used whe you're trying to predict continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = len(facet_list), activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More practically, we'll create a function that will build the model in one go. This will allow us to easily create new versions for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential([  \n",
    "        InputLayer(shape=(latitude, longitude, channels)),\n",
    "        Conv2D(filters = 16, kernel_size = (3,3), padding = 'same', activation = 'relu', name='Convolution_01'),\n",
    "        MaxPooling2D(pool_size = (2,2), name='Max_Pooling_01'),\n",
    "        Dropout(.25),\n",
    "        Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu', name='Convolution_02'),\n",
    "        BatchNormalization(name='Batch_Normalization'),\n",
    "        MaxPooling2D(pool_size = (2,2), name='Max_Pooling_02'),\n",
    "        Dropout(.25),\n",
    "        Flatten(),\n",
    "        Dense(units = 100, activation = 'relu', name='Hidden_Layer_01'),\n",
    "        Dense(units = 100, activation = 'relu', name='Hidden_Layer_02'),\n",
    "        Dropout(.25),\n",
    "        Dense(units = len(facet_list), activation = 'linear', name='Output')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look at the whole thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our model has a little over 95,000 parameters or weights to train (hence the need for a lot of images). Note that there are small set of non-trainable parameter from the normalization layer. \n",
    "\n",
    "##### We can also generate a simple visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, show_layer_activations=True,dpi=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The next step is to compile the model. This is an optimization function, a loss function and performance metric. A good option for the loss function in a regression problem is `mean_squared_error`, metrics allows the user to add aditional functions to monitor training. The optimizer implements the agorithmn which updates the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error', \n",
    "              metrics = [\"mean_absolute_error\"], \n",
    "              optimizer = RMSprop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "##### We'll now train the model for 50 epochs using the `fit()` method upon our built model. We specify:\n",
    "\n",
    "- `train_atmos`: The training subset of the atmospheric variables\n",
    "- `train_opg`: The training subset of the observed OPGs\n",
    "- `batch_size`: The number of samples or observation days used per gradient update.\n",
    "- `epochs`: The number of full training iterations.\n",
    "- `validation_data`: The validation subset of the atmospheric variables and observed OPGs\n",
    "\n",
    "##### This takes a few minutes to train (on my laptop). It's worth remembering what is going on here: the algorithm is reading in batches of 32 images, rescaling them, updating model weights through back propagation and then repeating the whole thing 50 times. As we previously defined a separate validation subset, this routine will calculate two losses:\n",
    "- The training loss. This is how accurately the model can predict OPG from the atmospheric 'images' that are being used to update the weights\n",
    "- The validation loss. This is how accurately the model can predict OPG from a set of atmospheric 'images' that are not used in updating the weights\n",
    "\n",
    "##### As the model continues to train, you should see the loss decrease for both of these, but will likely stabilize at a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "hist = model.fit(train_atmos, train_opg, # training data\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,                        # epochs\n",
    "                  validation_data = (val_atmos, val_opg), # validation data\n",
    "                  verbose = 1)                           # print progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now plot the evolution of the loss function and the actual versus predicted OPGs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(hist_obs, name):\n",
    "    # Define label sizes\n",
    "    label_size = 14\n",
    "    tick_size = 12\n",
    "    \n",
    "    # Create Figure\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(hist_obs.history['loss'], label='train')\n",
    "    plt.plot(hist_obs.history['val_loss'], label='valid')\n",
    "    plt.xlabel(\"Epochs\", fontsize=label_size)\n",
    "    plt.ylabel(\"Mean Squared Error\", fontsize=label_size)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save and show figure\n",
    "    path = \"../figures/\"\n",
    "    plt.savefig(f\"{path}eval_loss_{name}.png\", dpi=200, transparent=True,  bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_act_pred(modelx, hist_obs, name):\n",
    "    # Using our fitted model, predict OPGs from the testing subset.\n",
    "    predicted = modelx.predict(test_atmos)\n",
    "\n",
    "    # set zero OPGs to nan\n",
    "    actual = test_opg\n",
    "    actual[actual==0] = np.nan\n",
    "    predicted[predicted==0] = np.nan\n",
    "    \n",
    "    # Convert from standardized OPG to mm/m OPG\n",
    "    actual    = (test_opg * opg_std) + opg_mean\n",
    "    predicted = (predicted * opg_std) + opg_mean\n",
    "    \n",
    "    # Lets reshape our OPGs and remove NaN OPG days\n",
    "    actual    = np.reshape(actual, -1)\n",
    "    predicted = np.reshape(predicted, -1)\n",
    "    idx       = np.isnan(actual) # Identify days with OPG observations\n",
    "    actual    = actual[idx==False]\n",
    "    predicted = predicted[idx==False]\n",
    "    \n",
    "    # Formulate the x and y axis limits\n",
    "    max_val = np.max((np.max(predicted[:]), np.max(actual[:])))\n",
    "    min_val = np.min((np.min(predicted[:]), np.min(actual[:])))\n",
    "    max_val = 0.05\n",
    "    min_val = -0.05\n",
    "    \n",
    "    # Define plot colormap\n",
    "    cmap_gnuplot2 = ncm.cmap(\"MPL_gnuplot2\")\n",
    "    \n",
    "    # Define label sizes\n",
    "    label_size = 14\n",
    "    tick_size = 12\n",
    "    \n",
    "    # Create Figure\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    \n",
    "    # Formulate the heatmap variables\n",
    "    heatmap, xedges, yedges = np.histogram2d(np.reshape(actual, -1), np.reshape(predicted, -1), bins=100,\n",
    "                                 range=[[min_val, max_val],[min_val, max_val]])\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap_gnuplot2, norm=colors.LogNorm())\n",
    "\n",
    "    # Plot One-to-one line\n",
    "    plt.plot([xedges[0], xedges[-1]], [yedges[0], yedges[-1]], c='red')\n",
    "\n",
    "    # corr\n",
    "    corr = np.round(np.corrcoef(actual, predicted)[0,1]**2 , 2)\n",
    "    last_mse = np.round(hist_obs.history['val_loss'][-1], 2)\n",
    "    \n",
    "    # Add labels, gridlines, and colorbar\n",
    "    plt.xlabel(\"Actual\", fontsize=label_size)\n",
    "    plt.ylabel(\"Predicted\", fontsize=label_size)\n",
    "    plt.title(\"r^2 = \" + str(corr) + \", Error = \" + str(last_mse))\n",
    "    plt.grid(True)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Save and show figure\n",
    "    path = \"../figures/\"\n",
    "    plt.savefig(f\"{path}eval_act_pred_{name}.png\", dpi=200, transparent=True,  bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_loss(hist, 'model_01')\n",
    "plot_act_pred(model, hist, 'model_01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The plot shows a steep decline in both loss values, but no real improvement in the validation loss beyond epoch 20. It's quite likely that this model has overfit - become too tuned to the training data to allow prediction. There are a variety of ways we can avoid this, but a simple one is to slow the rate at which the model learns. We'll refit the model, but will add a `learning_rate` parameter to the compilation step. \n",
    "\n",
    "##### Additionally, `callbacks` can be added to the fitting process, in which the fit algorithm will monitor the validation loss. Here, when the validation loss does not improve after 5 epochs, the model stops training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', \n",
    "              metrics = [\"mean_absolute_error\"], \n",
    "              optimizer = RMSprop(learning_rate=1e-4))\n",
    "\n",
    "callback = [EarlyStopping(monitor='val_loss', patience=5, mode='min')]\n",
    "\n",
    "epochs = 100\n",
    "hist = model.fit(train_atmos, train_opg, # training data\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,                        # epochs\n",
    "                  validation_data = (val_atmos, val_opg), # validation data\n",
    "                  callbacks=[callback],                   # patience\n",
    "                  verbose = 1)                           # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist, 'model_02_adjust_lr')\n",
    "plot_act_pred(model, hist, 'model_02_adjust_lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The plot now shows a good evolution of the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does changing the stucture affect the training outcome?\n",
    "\n",
    "##### Along with the loss function, the structure of the CNN can be changed. I suggest seeing what happens when you do the following changes individually:\n",
    "1. `Change the Dense layers to have units=10`\n",
    "2. `Uncomment the additional MaxPooling2D and Conv2D layers`\n",
    "3. `Change the loss function to mean_absolute_error`\n",
    "4. `Set Dropout to zero`\n",
    "\n",
    "##### Pay mind to how changing these parameters affect the number of parameters to train, how long training time becomes, and the 2D histogram of actual versus predicted OPGs. Feel free to try your own edits to inspect how it affects the testing outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([  \n",
    "    InputLayer(shape=(latitude, longitude, channels)),\n",
    "    Conv2D(filters = 16, kernel_size = (3,3), padding = 'same', activation = 'relu'),\n",
    "    MaxPooling2D(pool_size = (2,2)),\n",
    "    Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu'),\n",
    "    # MaxPooling2D(pool_size = (2,2)),                                                    #2 <-- Comment in this line\n",
    "    # Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'),   #2 <-- Comment in this line\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size = (2,2)),\n",
    "    Dropout(0.25),                         #4 <-- Change the dropout from 0.25 to 0.0\n",
    "    Flatten(),\n",
    "    Dense(units = 100, activation = 'relu'),  #1 <-- Change the units of the Dense Layers from 100 to 10\n",
    "    Dense(units = 100, activation = 'relu'),  #1 <-- Change the units of the Dense Layers from 100 to 10\n",
    "    Dropout(0.25),                         #4 <-- Change the dropout from 0.25 to 0.0\n",
    "    Dense(units = len(facet_list), activation = 'linear') ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'mean_squared_error',          #3 <-- Change Loss Function Here from mean_squared_error to mean_absolute_error\n",
    "              metrics = [\"mean_absolute_error\", \"mean_squared_error\"], \n",
    "              optimizer = RMSprop(learning_rate=1e-4))\n",
    "\n",
    "epochs = 80\n",
    "batch_size = 32\n",
    "\n",
    "callback = [EarlyStopping(monitor='val_loss', patience=5, mode='min')]\n",
    "\n",
    "hist = model.fit(train_atmos, train_opg, \n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,                      \n",
    "                  validation_data = (val_atmos, val_opg),\n",
    "                  callbacks=[callback],\n",
    "                  verbose = 1)                     # <-- If you want to silence training, set to 0, if you want to watch it train, set to 1\n",
    "\n",
    "plot_loss(hist, 'model_03_change_architecture')\n",
    "plot_act_pred(model, hist, 'model_03_change_architecture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice the differences between outputs when changing the hyperparameters:\n",
    "1. `Change the Dense layers to have units=10`:\n",
    "  The number of trainable parameters decreases considerably from 95,678 to 13,714. Though the mean squared error reaches a minimum around 80 epochs, the OPG heatmap indicates a decrease in r^2 and an increase in error. Overall, the model is less able to predict more extreme OPGs. Possibiliy from too little parameters? Possibly the patience is too sensitive and it should train for longer?\n",
    "\n",
    "2. `Uncomment the additional MaxPooling2D and Conv2D layers`:\n",
    "  Additional MaxPooling2D and Conv2D layers decreased the number of trainable parameters from 95,614 to 75,774. Based on the mean squared error, the model seems to reach a minimum in error somewhere around 60 epochs. The addition of MaxPooling2D and Conv2D layers did not significantly improve or worsen the OPG predictions. More layers don't always improve the model?\n",
    "\n",
    "3. `Change the loss function to mean_absolute_error`:\n",
    "  The number of trainable parameters are consistent. Based on the plot of mean absolute error by epoch, the model tends to train very quickly in the beginning and trains slowly until ending. The OPG heatmap shows OPGs are more underpredicted than when using mean_squared_error, most likely due to the loss function. Mean squared error, since it is squared, forces the model to focus more on larger loss values.\n",
    "\n",
    "4. `Set Dropout to zero`:\n",
    "   The number of trainable parameters are consistent. Based on the plot of mean squared error by epoch, the model tends to train very quickly, possibly ending before epoch 20. This can cause the model to overfitting to the training data. \n",
    "  \n",
    "\n",
    "#### What you choose for hyperparameters and structure all depends on what your goals are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_env]",
   "language": "python",
   "name": "conda-env-cnn_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
