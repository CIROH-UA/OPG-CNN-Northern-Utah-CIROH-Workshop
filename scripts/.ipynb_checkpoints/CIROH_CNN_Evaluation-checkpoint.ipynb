{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe280deb-3c1c-4434-bf95-44bafc8e74aa",
   "metadata": {},
   "source": [
    "# CIROH Developers Conference: Hydrological Applications of ML\n",
    "## CNNs for Predicting Daily Orographic Precipitation Gradients for Atmospheric Downscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b219a-fe5f-4151-9f77-4dfbe81f9926",
   "metadata": {},
   "source": [
    "#### First, lets finalize our CNN for predicting OPGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068861e",
   "metadata": {},
   "source": [
    "Load in python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64e195b-1f6a-42d0-a5f8-713442194a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning library\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (InputLayer, Conv2D,\n",
    "                          Dense,\n",
    "                          ReLU,LeakyReLU,\n",
    "                          BatchNormalization, \n",
    "                          MaxPooling2D, \n",
    "                          Dropout,\n",
    "                          Flatten)\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import nclcmaps as ncm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37a161",
   "metadata": {},
   "source": [
    "Load in atmospheric and OPG data. Standardize the variables. Create atmos variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10cbc6cc-3061-424f-9e6d-e6baeb28cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the atmospheric data\n",
    "path      = \"../datasets/era5_atmos/\"\n",
    "IVT       = xr.open_dataset(f\"{path}IVT_sfc.nc\")\n",
    "precip    = xr.open_dataset(f\"{path}precip_sfc.nc\")*1000\n",
    "temp700   = xr.open_dataset(f\"{path}temp_700.nc\")-273.15\n",
    "uwinds700 = xr.open_dataset(f\"{path}uwnd_700.nc\")\n",
    "vwinds700 = xr.open_dataset(f\"{path}vwnd_700.nc\")\n",
    "hgt500    = xr.open_dataset(f\"{path}hgt_500.nc\")/9.81\n",
    "\n",
    "# load in the OPGs\n",
    "path = \"../datasets/facets_and_opgs/\"\n",
    "opg  = pd.read_csv(f\"{path}winter_northernUT_opg.csv\", index_col=0)\n",
    "facet_list = opg.columns.values\n",
    "opg = opg.values\n",
    "\n",
    "# Standardize\n",
    "IVT       = (IVT - IVT.mean(dim=\"time\")) / IVT.std(dim=\"time\")\n",
    "precip    = (precip - precip.mean(dim=\"time\")) / precip.std(dim=\"time\")\n",
    "temp700   = (temp700 - temp700.mean(dim=\"time\")) / temp700.std(dim=\"time\")\n",
    "uwinds700 = (uwinds700 - uwinds700.mean(dim=\"time\")) / uwinds700.std(dim=\"time\")\n",
    "vwinds700 = (vwinds700 - vwinds700.mean(dim=\"time\")) / vwinds700.std(dim=\"time\")\n",
    "hgt500    = (hgt500 - hgt500.mean(dim=\"time\")) / hgt500.std(dim=\"time\")\n",
    "\n",
    "opg_mean = np.nanmean(opg, axis=0)\n",
    "opg_std  = np.nanstd(opg, axis=0)\n",
    "opg = (opg - opg_mean) / opg_std\n",
    "opg[np.isnan(opg)] = 0\n",
    "\n",
    "# Combine the atmospheric data into one array\n",
    "atmos = np.concatenate((IVT.IVT.values[...,np.newaxis], \n",
    "                        precip.precip.values[...,np.newaxis],\n",
    "                        temp700.temp.values[...,np.newaxis],\n",
    "                        uwinds700.uwnd.values[...,np.newaxis],\n",
    "                        vwinds700.vwnd.values[...,np.newaxis],\n",
    "                        hgt500.hgt.values[...,np.newaxis]), axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232f527",
   "metadata": {},
   "source": [
    "Create varaiables containing size of atmospheric 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d894e625-6faa-49be-baf6-98d82127af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude  = 19\n",
    "longitude = 27\n",
    "channels  = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c48b6",
   "metadata": {},
   "source": [
    "Shuffle observation days and split the data into training, testing, and validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fccf9580-a088-4333-ad7d-a6676949908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406, 19, 27, 6)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "rand_ind = np.random.permutation(np.arange(np.shape(atmos)[0]))\n",
    "\n",
    "train_atmos = atmos[rand_ind[:1896], ...]\n",
    "test_atmos  = atmos[rand_ind[1896:2302], ...]\n",
    "val_atmos   = atmos[rand_ind[2302:], ...]\n",
    "\n",
    "train_opg = opg[rand_ind[:1896], ...]\n",
    "test_opg  = opg[rand_ind[1896:2302], ...]\n",
    "val_opg   = opg[rand_ind[2302:], ...]\n",
    "\n",
    "print(np.shape(test_atmos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a5c8b",
   "metadata": {},
   "source": [
    "Define the training batch size (i.e., the number of images to import for any update step) and the CNN stucture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e34bec-fd5c-47dc-8dd4-c9158eda0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([  \n",
    "        InputLayer(shape=(latitude, longitude, channels)),\n",
    "        Conv2D(filters = 16, kernel_size = (3,3), padding = 'same', activation = 'relu'),\n",
    "        MaxPooling2D(pool_size = (2,2)),\n",
    "        Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size = (2,2)),\n",
    "        Dropout(.25),\n",
    "        Flatten(),\n",
    "        Dense(units = 100, activation = 'relu'),\n",
    "        Dense(units = 100, activation = 'relu'),\n",
    "        Dropout(.25),\n",
    "        Dense(units = len(facet_list))\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3f606",
   "metadata": {},
   "source": [
    "Create and compile the CNN, then fit to the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b36bbfd5-b171-48dc-b286-45813df6d3a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Unrecognized keyword arguments:', dict_keys(['shape']))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c1f8919a25a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m model.compile(loss = 'mean_squared_error', \n\u001b[0;32m      5\u001b[0m               \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"mean_absolute_error\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-471adbd557b9>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     model = Sequential([  \n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\3.8\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized keyword arguments:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mragged\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Unrecognized keyword arguments:', dict_keys(['shape']))"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', \n",
    "              metrics = [\"mean_absolute_error\"], \n",
    "              optimizer = RMSprop(learning_rate=1e-4))\n",
    "\n",
    "epochs = 80\n",
    "hist = model.fit(train_atmos, train_opg, # training data\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,                        # epochs\n",
    "                  validation_data = (val_atmos, val_opg), # validation data\n",
    "                  verbose = 1)                           # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3dba6-8c30-46c3-9803-f830f04c7b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beaf6c2-7d3a-409f-bf07-13a98312a98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8c19f-3242-44b0-ba10-7dad468ec182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8a28a07-0147-47a7-8169-96fe1c56788a",
   "metadata": {},
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaab53-4496-4456-92ce-c5a24d5c59a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4df03eb-c7bc-4bf4-ba03-3804043c55fe",
   "metadata": {},
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b637360-1bcc-4698-a2c6-8f1a61e31204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f52c98e7-21f4-4292-a21d-ee2ab74f3a20",
   "metadata": {},
   "source": [
    "### Downscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00b00b-e612-450a-a448-79eb1deab638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
