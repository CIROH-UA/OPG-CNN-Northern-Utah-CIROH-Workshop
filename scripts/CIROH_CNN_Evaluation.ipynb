{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe280deb-3c1c-4434-bf95-44bafc8e74aa",
   "metadata": {},
   "source": [
    "# CIROH Developers Conference: Hydrological Applications of ML\n",
    "### CNNs for Predicting Daily Orographic Precipitation Gradients for Atmospheric Downscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8890f-f872-48d3-874a-c5b3d27fea7a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise, we'll build another convolutional neural network (CNN) for regression of orographic precipitation gradients of Northern Utah. Then we'll evaluate it using feature maps, Grad-CAMs, and finally downscale precipitation.\n",
    "\n",
    "The Explainable AI techniques employed here are:\n",
    "\n",
    "- Feature Maps: Convolutions, max pooling, and batch normalization transforms the input images after each layer. The outputted images from these transformations are called Feature Maps.\n",
    "- Grad-CAM (Gradient-weighted Class Activation Maps): This technique is used to help with \"debugging\" the decision process of the CNN. It creates heatmaps indicating influential regions within an image for the CNNs prediction. The heatmaps are formulated from the gradients from an output node to each gridpoint in the last convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b219a-fe5f-4151-9f77-4dfbe81f9926",
   "metadata": {},
   "source": [
    "## First, lets build and train the CNN we'll evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068861e",
   "metadata": {},
   "source": [
    "##### Load in python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e195b-1f6a-42d0-a5f8-713442194a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning library for tensorflow, a function to \n",
    "# sequentially build a CNN architecture, the CNN layers, \n",
    "# and a function to stop the CNN training early\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Input, Conv2D, Dense, ReLU, BatchNormalization, \n",
    "                                      MaxPooling2D, Dropout, Flatten)\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # To load in CSV data files\n",
    "import numpy as np\n",
    "import xarray as xr # To load and manipulate NetCDF data files\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeat\n",
    "\n",
    "# You can also load in other python scripts that contain \n",
    "# functions like a Python Library. This one contains a list \n",
    "# of Colormaps for plotting\n",
    "import nclcmaps as ncm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37a161",
   "metadata": {},
   "source": [
    "##### Load in the OPG and atmospheric data. Standardize the variables. Create atmospheric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cbc6cc-3061-424f-9e6d-e6baeb28cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the OPGs\n",
    "path = \"../datasets/facets_and_opgs/\"\n",
    "opg  = pd.read_csv(f\"{path}winter_northernUT_opg.csv\", index_col=0)\n",
    "facet_list = opg.columns.values\n",
    "opg = opg.values\n",
    "\n",
    "# Standardized the OPG data\n",
    "opg_mean = np.nanmean(opg, axis=0)\n",
    "opg_sd  = np.nanstd(opg, axis=0)\n",
    "opg = (opg - opg_mean) / opg_sd\n",
    "opg[np.isnan(opg)] = 0\n",
    "\n",
    "# load in the atmospheric data\n",
    "path      = \"../datasets/era5_atmos/\"\n",
    "IVT       = xr.open_dataset(f\"{path}IVT_sfc.nc\")\n",
    "precip    = xr.open_dataset(f\"{path}precip_sfc.nc\")*1000\n",
    "temp700   = xr.open_dataset(f\"{path}temp_700.nc\")-273.15\n",
    "uwinds700 = xr.open_dataset(f\"{path}uwnd_700.nc\")\n",
    "vwinds700 = xr.open_dataset(f\"{path}vwnd_700.nc\")\n",
    "hgt500    = xr.open_dataset(f\"{path}hgt_500.nc\")/9.81\n",
    "\n",
    "# Extract the latitudes and longitudes of the ERA5\n",
    "lat_ERA5 = hgt500['latitude']\n",
    "lon_ERA5 = hgt500['longitude']\n",
    "\n",
    "# Standardize the atmospheric data\n",
    "IVT_std       = (IVT - IVT.mean(dim=\"time\")) / IVT.std(dim=\"time\")\n",
    "precip_std    = (precip - precip.mean(dim=\"time\")) / precip.std(dim=\"time\")\n",
    "temp700_std   = (temp700 - temp700.mean(dim=\"time\")) / temp700.std(dim=\"time\")\n",
    "uwinds700_std = (uwinds700 - uwinds700.mean(dim=\"time\")) / uwinds700.std(dim=\"time\")\n",
    "vwinds700_std = (vwinds700 - vwinds700.mean(dim=\"time\")) / vwinds700.std(dim=\"time\")\n",
    "hgt500_std    = (hgt500 - hgt500.mean(dim=\"time\")) / hgt500.std(dim=\"time\")\n",
    "\n",
    "# Combine the atmospheric data into one array\n",
    "atmos_std = np.concatenate((IVT_std.IVT.values[...,np.newaxis], \n",
    "                        precip_std.precip.values[...,np.newaxis],\n",
    "                        temp700_std.temp.values[...,np.newaxis],\n",
    "                        uwinds700_std.uwnd.values[...,np.newaxis],\n",
    "                        vwinds700_std.vwnd.values[...,np.newaxis],\n",
    "                        hgt500_std.hgt.values[...,np.newaxis]), axis=3)\n",
    "\n",
    "print(np.shape(atmos_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232f527",
   "metadata": {},
   "source": [
    "##### Create varaiables containing size of atmospheric 'images'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894e625-6faa-49be-baf6-98d82127af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude  = 19\n",
    "longitude = 27\n",
    "channels  = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c48b6",
   "metadata": {},
   "source": [
    "##### Shuffle observation days and split the data into training, testing, and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf9580-a088-4333-ad7d-a6676949908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index to randomize the dataset\n",
    "np.random.seed(42)\n",
    "rand_ind = np.random.permutation(np.arange(np.shape(atmos_std)[0]))\n",
    "\n",
    "# Extract the training, testing, and validation subsets\n",
    "train_atmos = atmos_std[rand_ind[:1896], ...]\n",
    "test_atmos  = atmos_std[rand_ind[1896:2302], ...]\n",
    "val_atmos   = atmos_std[rand_ind[2302:], ...]\n",
    "\n",
    "train_opg = opg[rand_ind[:1896], ...]\n",
    "test_opg  = opg[rand_ind[1896:2302], ...]\n",
    "val_opg   = opg[rand_ind[2302:], ...]\n",
    "\n",
    "print(np.shape(test_atmos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a5c8b",
   "metadata": {},
   "source": [
    "##### Define the training batch size (i.e., the number of images to import for any update step) and the CNN stucture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e34bec-fd5c-47dc-8dd4-c9158eda0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([  \n",
    "        Input(shape=(latitude, longitude, channels), name='input'),\n",
    "        Conv2D(filters = 16, kernel_size = (3,3), padding = 'same', activation = 'relu', name='Convolution_01'),\n",
    "        MaxPooling2D(pool_size = (2,2), name='Max_Pooling_01'),\n",
    "        Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu', name='Convolution_02'),\n",
    "        BatchNormalization(name='Batch_Normalization'),\n",
    "        MaxPooling2D(pool_size = (2,2), name='Max_Pooling_02'),\n",
    "        Dropout(.25),\n",
    "        Flatten(),\n",
    "        Dense(units = 100, activation = 'relu', name='Hidden_Layer_01'),\n",
    "        Dense(units = 100, activation = 'relu', name='Hidden_Layer_02'),\n",
    "        Dropout(.25),\n",
    "        Dense(units = len(facet_list), name='Output')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3f606",
   "metadata": {},
   "source": [
    "##### Create and compile the CNN, then fit to the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36bbfd5-b171-48dc-b286-45813df6d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', \n",
    "              metrics = [\"mean_absolute_error\"], \n",
    "              optimizer = RMSprop(learning_rate=1e-4))\n",
    "\n",
    "callback = [EarlyStopping(monitor='val_loss', patience=5, mode='min')]\n",
    "epochs = 80\n",
    "hist = model.fit(train_atmos, train_opg, # training data\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,                        # epochs\n",
    "                  validation_data = (val_atmos, val_opg), # validation data\n",
    "                  callbacks=[callback],                   # patience\n",
    "                  verbose = 1)                           # print progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fb536-1df1-46aa-811a-791eb1b6c4f2",
   "metadata": {},
   "source": [
    "## Initial Evaluation of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beaf6c2-7d3a-409f-bf07-13a98312a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the training and validation loss at each epoch\n",
    "def plot_loss(hist_obs):\n",
    "    # Create Figure\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(hist_obs.history['loss'], label='train')\n",
    "    plt.plot(hist_obs.history['val_loss'], label='valid')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save and show figure\n",
    "    path = \"../figures/\"\n",
    "    plt.savefig(f\"{path}eval_loss_model_04.png\", dpi=200, transparent=True,  bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot the actual vs predicted OPGs\n",
    "def plot_act_pred(modelx, hist_obs):\n",
    "    # Using our fitted model, predict OPGs from the testing subset.\n",
    "    predicted = modelx.predict(test_atmos)\n",
    "\n",
    "    # set zero OPGs to nan\n",
    "    actual = test_opg\n",
    "    actual[actual==0] = np.nan\n",
    "    predicted[predicted==0] = np.nan\n",
    "    \n",
    "    # Convert from standardized OPG to mm/m OPG\n",
    "    actual    = (test_opg * opg_sd) + opg_mean\n",
    "    predicted = (predicted * opg_sd) + opg_mean\n",
    "    \n",
    "    # Since days without OPG observations are set to zero, lets reshape our OPGs and remove zero OPG days\n",
    "    actual    = np.reshape(actual, -1)\n",
    "    predicted = np.reshape(predicted, -1)\n",
    "    idx       = np.isnan(actual) # Identify days without OPG observations\n",
    "    actual    = actual[idx==False]\n",
    "    predicted = predicted[idx==False]\n",
    "    \n",
    "    # Formulate the x and y axis limits\n",
    "    max_val = np.max((np.max(predicted[:]), np.max(actual[:])))\n",
    "    min_val = np.min((np.min(predicted[:]), np.min(actual[:])))\n",
    "    max_val = 0.05\n",
    "    min_val = -0.05\n",
    "    \n",
    "    # Define plot colormap\n",
    "    cmap_gnuplot2 = ncm.cmap(\"MPL_gnuplot2\")\n",
    "    \n",
    "    # Define label sizes\n",
    "    label_size = 16\n",
    "    tick_size = 14\n",
    "    \n",
    "    # Create Figure\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    \n",
    "    # Formulate the heatmap variables\n",
    "    heatmap, xedges, yedges = np.histogram2d(np.reshape(actual, -1), np.reshape(predicted, -1), bins=100,\n",
    "                                 range=[[min_val, max_val],[min_val, max_val]])\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap_gnuplot2, norm=colors.LogNorm())\n",
    "\n",
    "    # Plot One-to-one line\n",
    "    plt.plot([xedges[0], xedges[-1]], [yedges[0], yedges[-1]], c='red')\n",
    "    \n",
    "    # corr\n",
    "    corr = np.round(np.corrcoef(actual, predicted)[0,1]**2 , 2)\n",
    "    last_mse = np.round(hist_obs.history['val_loss'][-1], 2)\n",
    "    \n",
    "    # Add labels, gridlines, and colorbar\n",
    "    plt.xlabel(\"Actual\", fontsize=label_size)\n",
    "    plt.ylabel(\"Predicted\", fontsize=label_size)\n",
    "    plt.title(\"r^2 = \" + str(corr) + \", Error = \" + str(last_mse))\n",
    "    plt.grid(True)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Save and show figure\n",
    "    path = \"../figures/\"\n",
    "    plt.savefig(f\"{path}eval_act_pred_model_04.png\", dpi=200, transparent=True,  bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_loss(hist)\n",
    "plot_act_pred(model, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832cddd-05aa-4c41-89e4-97ddfad56a9b",
   "metadata": {},
   "source": [
    "##### From the training and validation loss plot, we should see a smooth decrease in mean squared error until the `EarlyStopping` function ended training. \n",
    "\n",
    "##### The actual vs predicted OPG plot indicates some underpredicted of high OPGs and overprediction of low OPGs. Though as shown with the log-scale colormap, a significant majority of OPGs are near- or just above zero. With less examples of extreme OPGs to train from, extreme values are more difficult to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a28a07-0147-47a7-8169-96fe1c56788a",
   "metadata": {},
   "source": [
    "## Feature Maps - Convolutional, Max Pooling Layers, and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57256ccb-e0fc-4034-b615-6cc39c84a8d1",
   "metadata": {},
   "source": [
    "##### Let's take a closer look at how the convolutional, max-pooling, and batch normalization process by visually by plotting feature maps. Here, we will index the same days from the CIROH_Datasets.ipynb script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31126e73-7cb5-41ae-8af2-0bb8e11f5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dates to plot\n",
    "dates = np.array(['2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10', '2017-01-11', '2017-01-12'], dtype='datetime64')\n",
    "\n",
    "# Find index of these dates within the Xarray\n",
    "idx = IVT.time.dt.date.isin(dates)\n",
    "\n",
    "# Index Atmospheric Variables\n",
    "atmos_subset = atmos_std[idx,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581ee2c-2bc1-472c-b5b8-40552b499604",
   "metadata": {},
   "source": [
    "##### Below is the function that formulates feature maps from predictor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2a23e-dc43-4cf7-9d6e-d6747c9bd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_maps(layer_name, inputs):\n",
    "    # Create a sudo-model that takes the input images and outputs \n",
    "    # the layer desired from the model already trained.\n",
    "    feature_model = tf.keras.models.Model(model.inputs, [model.get_layer(layer_name).output])\n",
    "\n",
    "    # Then compute the gradient of the top predicted class for our\n",
    "    # input image with resepct to the activations of the layer\n",
    "    LAYER = feature_model.predict(inputs)\n",
    "\n",
    "    # Standardize, then normalize the feature maps\n",
    "    LAYER = (LAYER - np.mean(LAYER)) / np.std(LAYER)\n",
    "    LAYER[LAYER < 0] = 0\n",
    "    LAYER /= np.max(LAYER)\n",
    "\n",
    "    return LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550413dd-cc8d-451a-a275-1665b65421d6",
   "metadata": {},
   "source": [
    "##### Now lets loop through only 1 day of input data, call the feature_maps() function, and plot the layers. This function runs quite slow, but if you want to see more dates, uncomment `for dayx in range(np.shape(atmos_subset)[0]):`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbebba-d602-45b9-9715-fab3334930f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [-119, -106, 36, 45] # set lat/lon extent of plots\n",
    "datacrs = ccrs.PlateCarree()  # Map Projections\n",
    "\n",
    "for dayx in range(1,2):                            # <-- Keep to loop through only 1 date\n",
    "# for dayx in range(np.shape(atmos_subset)[0]):    # <-- Uncomment to see feature maps from all 6 dates\n",
    "    # Index the atmospheric variables from looped day\n",
    "    inputs = [atmos_subset[[dayx],:,:,:]]  \n",
    "\n",
    "    # Pull date name\n",
    "    datex = np.datetime_as_string(dates[dayx], unit=\"D\")\n",
    "    \n",
    "    # Loop through each layer\n",
    "    for lx in range(len(model.layers)):\n",
    "        # Pull layer name from model\n",
    "        layer = model.layers[lx] \n",
    "        \n",
    "        if ('Conv' not in layer.name) and ('Max' not in layer.name) and ('Batch' not in layer.name):\n",
    "           continue # skip if not convolutional, max pooling or batch normalization layer\n",
    "\n",
    "        ########## FORMULATE FEATURE MAPS FROM THE PREDICTORS ##########\n",
    "        LAYER = feature_maps(layer.name, inputs)\n",
    "\n",
    "        ########## PLOT FEATURE MAPS ##########\n",
    "        # Create mesh grid of lat/lons to plot the feature maps onto\n",
    "        lats = np.linspace(np.min(lat_ERA5), np.max(lat_ERA5), num=np.shape(LAYER)[1], endpoint=True)\n",
    "        lons = np.linspace(np.min(lon_ERA5), np.max(lon_ERA5), num=np.shape(LAYER)[2], endpoint=True)\n",
    "        loni, lati = np.meshgrid(lons, lats)\n",
    "    \n",
    "        # Create the size of the figure and subplots\n",
    "        y_axis = 2\n",
    "        x_axis = np.ceil(np.shape(LAYER)[3]/y_axis).astype('int')\n",
    "        \n",
    "        # Plot projection\n",
    "        projex = ccrs.Mercator(central_longitude=np.mean(lons))\n",
    "    \n",
    "        # Create figure and the sub-axes\n",
    "        fig, ax = plt.subplots(nrows=y_axis, ncols=x_axis, figsize=(x_axis*4.25, y_axis*4),\n",
    "                               subplot_kw={'projection': projex})\n",
    "        \n",
    "        ax_count = 0 # value to count the feature maps\n",
    "        for row in range(y_axis):\n",
    "            for col in range(x_axis):\n",
    "                if ax_count < np.shape(LAYER)[3]:                    \n",
    "                    # Plot Feature Map                    \n",
    "                    feat_c = ax[row, col].pcolor(loni, lati, np.squeeze(LAYER[0, :, :, ax_count]), \n",
    "                                        clim=[0, 1], zorder=1, cmap='viridis', transform=datacrs)\n",
    "                    \n",
    "                    # Plot lakes, state boundaries, and set extent of maps\n",
    "                    ax[row,col].add_feature(cfeat.LAKES.with_scale('10m'), facecolor=\"lightsteelblue\", linewidth=3)\n",
    "                    ax[row,col].add_feature(cfeat.STATES.with_scale('10m'), edgecolor=\"white\", linewidth=3)\n",
    "                    ax[row,col].set_extent(extent)\n",
    "\n",
    "                    # Set edgecolor to white\n",
    "                    for spine in ax[row,col].spines.values(): spine.set_edgecolor(\"white\")\n",
    "\n",
    "                    # Add to count variable\n",
    "                    ax_count += 1\n",
    "\n",
    "                # After we plot every feature map, extra axes are removed from plot\n",
    "                elif ax_count >= np.shape(LAYER)[3]: ax[row, col].axis('off')\n",
    "\n",
    "        # Pull layer name, then create title\n",
    "        name = layer.name\n",
    "        plt.suptitle(datex + \" \" + name.replace(\"_\", \" \"), fontsize=4.5*x_axis, y=0.86+(x_axis/100), weight='bold')\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar_feat = fig.add_axes([0.91, 0.2, 0.01, 0.6])\n",
    "        cb_at = fig.colorbar(feat_c, cax=cbar_feat, #ticks=np.arange(-6,8,2),\n",
    "                              pad=0.0, aspect=15, fraction=0.032)\n",
    "        cb_at.ax.tick_params(labelsize=3.5*x_axis)\n",
    "\n",
    "        # Tighten layout\n",
    "        #plt.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "        # Save and show figure\n",
    "        path = \"../figures/\"\n",
    "        plt.savefig(f\"{path}feature_maps_{datex}_{name}.png\", dpi=200, transparent=True,  bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa2062-c2a0-4491-904a-b49bdc1bd076",
   "metadata": {},
   "source": [
    "##### Above are the feature maps for the convolutional, max-pooling, and batch normalization layers. Purple indicates the lowest values and yellow indicates the highest. Each type of layer are evaluated as:\n",
    "\n",
    "- Convolutional: Regions of lighter colors indicate where the convolutional filters highlight, the patterns the CNN learned were important for OPG prediction.\n",
    "\n",
    "- Max-Pooling: Shows how the max-pooling layer coarsens the images.\n",
    "\n",
    "- Batch Normalization: This layer normalizes the current batch, which helps the CNN generalize the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df03eb-c7bc-4bf4-ba03-3804043c55fe",
   "metadata": {},
   "source": [
    "## Gradient-weighted Class Activation Maps (Grad-CAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e604ea-b4ea-42ee-af54-9d57f51784b5",
   "metadata": {},
   "source": [
    "##### Grad-CAMs identify the influential regions within an image associated with a CNN's prediction to aid in evaluating the CNN. Effectively, Grad-CAMs are heatmaps highlighting where the CNN focuses on within each input image during a prediction. These heatmaps are the formulated gradinets form the predicted value to a layer within the CNN, with stronger gradients indicating stronger influence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b637360-1bcc-4698-a2c6-8f1a61e31204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dates to plot\n",
    "dates = np.array(['2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10', '2017-01-11', '2017-01-12'], dtype='datetime64')\n",
    "\n",
    "# Find index of these dates within the Xarray\n",
    "idx = IVT.time.dt.date.isin(dates)\n",
    "\n",
    "# Index standardized atmospheric variables for prediction\n",
    "atmos_subset = atmos_std[idx,...]\n",
    "\n",
    "# Index original atmospheric variables for plotting\n",
    "IVT_subset       = IVT.sel(time=slice(dates[0], dates[-1]))\n",
    "precip_subset    = precip.sel(time=slice(dates[0], dates[-1]))\n",
    "temp700_subset   = temp700.sel(time=slice(dates[0], dates[-1]))\n",
    "uwinds700_subset = uwinds700.sel(time=slice(dates[0], dates[-1]))\n",
    "vwinds700_subset = vwinds700.sel(time=slice(dates[0], dates[-1]))\n",
    "hgt500_subset    = hgt500.sel(time=slice(dates[0], dates[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2655a4-72b1-4d31-92ca-6661a9119b6b",
   "metadata": {},
   "source": [
    "##### Below is the function to formulate the Grad-CAMs from the predictor dataset. The steps of this function differ from others becasue it creates a composite of all output nodes. Most often, Grad-CAMs are formulated for CNNs predicting classifications, in which the output of individual nodes, like the target value, is most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e2be6-0dca-4930-99cc-beccce40d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(inputs):\n",
    "    lastConvName  = 'Convolution_02'\n",
    "    outputDenseName = 'Output'\n",
    "    \n",
    "    # Create a sudo-model that takes the input images and outputs \n",
    "    # the activation of the last convolutional layer as well as the predictions.\n",
    "    gradCAM_model = tf.keras.models.Model(model.inputs, [model.get_layer(lastConvName).output, model.get_layer(outputDenseName).output])\n",
    "    \n",
    "    # Create empty list to hold activation maps\n",
    "    heatmap = []\n",
    "    \n",
    "    for output_node in range(len(facet_list)):\n",
    "    \n",
    "        # Compute the activation maps from the output node to the input image with \n",
    "        # respect to the activations of the last convolutional layer\n",
    "        with tf.GradientTape() as tape:\n",
    "            last_conv_layer_output, preds = gradCAM_model(inputs)\n",
    "            output_node_value = preds[:, output_node]\n",
    "        \n",
    "        # Formulate the gradient from the output value and the last convolutional layer's feature maps\n",
    "        grads = tape.gradient(output_node_value, last_conv_layer_output)\n",
    "    \n",
    "        # Formulate the mean intensity of the gradients within each feature map\n",
    "        # i.e., How important is each feature map?\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "        # Reduce axes of array because the array shape is: (1, 9, 13, 32)\n",
    "        last_conv_layer_output = tf.squeeze(last_conv_layer_output)\n",
    "    \n",
    "        # Using matrix multiplication, multiply the importance of each \n",
    "        # activation map by the mean gradient on each activation map\n",
    "        heatmap_output_node = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    \n",
    "        # Because we are formulating a Grad-CAM for ALL output nodes to then combine \n",
    "        # all Grad-CAMs into one plot, a novel process, we will save the heatmap to a list\n",
    "        heatmap.append(np.array(heatmap_output_node))\n",
    "    \n",
    "    # With all the heatmaps saved from all output nodes, lets concatenate them into one array\n",
    "    heatmap = np.concatenate(heatmap, axis=2)\n",
    "    \n",
    "    # Sift out the max activation from all activation maps\n",
    "    heatmap = np.max(heatmap, axis=2)\n",
    "    \n",
    "    # Scale values to maximum of 1 and set all negative values to zero\n",
    "    heatmap = heatmap / np.max(heatmap[:])\n",
    "    heatmap[heatmap < 0] = 0\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f68de-9c61-4fb5-bb45-c73694a3d440",
   "metadata": {},
   "source": [
    "##### Here, lets loop through each date, call the Grad-CAM function, and plot a composite Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c258a6-8407-4de0-ad69-753a8dd77b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [-119, -106, 36, 45] # set lat/lon extent of plots\n",
    "datacrs = ccrs.PlateCarree()  # Map Projections\n",
    "\n",
    "# Loop through each day\n",
    "for dayx in range(np.shape(atmos_subset)[0]):\n",
    "    # Index the atmospheric variables from looped day\n",
    "    inputs = [atmos_subset[[dayx],:,:,:]]  \n",
    "\n",
    "    # Pull date name\n",
    "    datex = np.datetime_as_string(dates[dayx], unit=\"D\")\n",
    "\n",
    "    ########## FORMULATE GRAD-CAMS FOR ALL NODES ##########\n",
    "    heatmap = grad_cam(inputs)\n",
    "\n",
    "    ########## PLOT GRAD-CAM ONTOP OF ATMOSPHERIC VARIABLES ##########\n",
    "    \n",
    "    # Create mesh grid of lat/lons to plot the feature maps onto\n",
    "    lats = np.linspace(np.min(lat_ERA5), np.max(lat_ERA5), num=np.shape(heatmap)[0], endpoint=True)\n",
    "    lons = np.linspace(np.min(lon_ERA5), np.max(lon_ERA5), num=np.shape(heatmap)[1], endpoint=True)\n",
    "    loni, lati = np.meshgrid(lons, lats)\n",
    "\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2, 3, figsize = (6, 4), subplot_kw={'projection': projex})\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Plot contourf and contour\n",
    "    ivt_cbar = axs[0, 0].contourf(IVT_subset.longitude, IVT_subset.latitude, IVT_subset.IVT.values[dayx,:,:], \n",
    "                                  cmap=ncm.cmap('WhiteYellowOrangeRed'), levels=np.arange(100, 500, 50), \n",
    "                                  extend='max', transform=datacrs)\n",
    "    cs       = axs[0, 1].contour(hgt500_subset.longitude, hgt500_subset.latitude, hgt500_subset.hgt.values[dayx,:,:], \n",
    "                                 colors='black', levels=np.arange(5160, 5880, 60), transform=datacrs)\n",
    "    pr_cbar  = axs[0, 2].contourf(precip_subset.longitude, precip_subset.latitude, precip_subset.precip.values[dayx,:,:], \n",
    "                                  cmap=ncm.cmapDiscrete('prcp_2', indexList=np.arange(1,12)), \n",
    "                                  levels=np.arange(0.5, 10.5, 0.5), extend='max', transform=datacrs)\n",
    "    u_cbar   = axs[1, 0].contourf(uwinds700_subset.longitude, uwinds700_subset.latitude, uwinds700_subset.uwnd.values[dayx,:,:], \n",
    "                                  cmap=ncm.cmap('MPL_RdBu', True), levels=np.arange(-30, 30, 5), transform=datacrs)\n",
    "    v_cbar   = axs[1, 1].contourf(vwinds700_subset.longitude, vwinds700_subset.latitude, vwinds700_subset.vwnd.values[dayx,:,:], \n",
    "                                  cmap=ncm.cmap('MPL_RdBu', True), levels=np.arange(-30, 30, 5), transform=datacrs)\n",
    "    tmp_cbar = axs[1, 2].contourf(temp700_subset.longitude, temp700_subset.latitude, temp700_subset.temp.values[dayx,:,:], \n",
    "                                  cmap=ncm.cmap('NCV_jet'), levels=np.arange(-24, 24, 2), transform=datacrs)\n",
    "\n",
    "    # Set title\n",
    "    axs[0, 0].set_title(\"IVT ($kg m^{-1} s^{-1}$)\")  \n",
    "    axs[0, 1].set_title(\"500 hPa Geo. Hgts\")\n",
    "    axs[0, 2].set_title(\"Accum. Precip. ($mm$)\")\n",
    "    axs[1, 0].set_title(\"700 hPa U-Wind ($m/s$)\")\n",
    "    axs[1, 1].set_title(\"700 hPa V-Wind ($m/s$)\")\n",
    "    axs[1, 2].set_title(\"700 hPa Temp. ($^\\circ$C)\")   \n",
    "\n",
    "    # Plot 500 hPa geopotential height contour labels\n",
    "    axs[0, 1].clabel(cs, fmt=\"%d\", inline = True)\n",
    "\n",
    "    # Add colorbars\n",
    "    cx_ivt = fig.add_axes([-0.01, 0.55, 0.03, 0.40])\n",
    "    fig.colorbar(ivt_cbar, cax=cx_ivt, ticks=np.arange(100, 500, 100), orientation='vertical', location='left')\n",
    "\n",
    "    cx_u = fig.add_axes([-0.01, 0.05, 0.03, 0.40])\n",
    "    fig.colorbar(u_cbar, cax=cx_u, ticks=np.arange(-30, 30+1, 10), orientation='vertical', location='left')\n",
    "\n",
    "    cx_tmp = fig.add_axes([0.98, 0.05, 0.03, 0.40])\n",
    "    fig.colorbar(tmp_cbar, cax=cx_tmp, ticks=np.arange(-24, 24, 8), orientation='vertical', location='right')\n",
    "\n",
    "    cx_pr = fig.add_axes([0.98, 0.55, 0.03, 0.40])\n",
    "    fig.colorbar(pr_cbar, cax=cx_pr, ticks=np.arange(0.5, 12, 2), orientation='vertical', location='right')\n",
    "\n",
    "    # Add Grad-CAMs\n",
    "    for row in range(2):\n",
    "        for col in range(3):\n",
    "            axs[row, col].contourf(loni, lati, heatmap, colors='none', transform=datacrs,\n",
    "                                   hatches=[None, None, None, '..', 'oo'], levels=[0, 0.2, 0.4, 0.6, 0.8, 1.00])\n",
    "    \n",
    "    # Add wind barbs\n",
    "    #u700 = uwinds700_subset.uwnd.values[dayx,::5,::5] * 1.94384  \n",
    "    #v700 = vwinds700_subset.vwnd.values[dayx,::5,::5] * 1.94384\n",
    "    #axs[1, 0].barbs(uwinds700_subset.longitude[::5], uwinds700_subset.latitude[::5], \n",
    "    #                u700, v700, length=6, pivot='middle', transform=datacrs)\n",
    "    #axs[1, 1].barbs(uwinds700_subset.longitude[::5], uwinds700_subset.latitude[::5], \n",
    "    #                u700, v700, length=6, pivot='middle', transform=datacrs)\n",
    "    \n",
    "    # Add Cartogrphy\n",
    "    for row in range(2):\n",
    "        for col in range(3):\n",
    "            axs[row,col].add_feature(cfeat.LAND, facecolor=\"burlywood\")\n",
    "            axs[row,col].add_feature(cfeat.RIVERS.with_scale('10m'), edgecolor=\"royalblue\")\n",
    "            axs[row,col].add_feature(cfeat.LAKES.with_scale('10m'), facecolor=\"dodgerblue\")\n",
    "            axs[row,col].add_feature(cfeat.STATES.with_scale('10m'), edgecolor=\"saddlebrown\")\n",
    "            axs[row,col].set_extent(extent)\n",
    "\n",
    "    # adjust plot\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "    datex = np.datetime_as_string(IVT_subset.time.values[dayx], unit=\"D\")\n",
    "    plt.suptitle(datex, y=1.06, weight='bold')\n",
    "\n",
    "    # Save and show figure\n",
    "    path = \"../figures/\"\n",
    "    plt.savefig(f\"{path}grad_cam_{datex}.png\", dpi=200, transparent=True,  bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02591cc-cd18-4f9a-8246-8c7fbde3c44e",
   "metadata": {},
   "source": [
    "##### The above plots show the composite Grad-CAMs for 6 dates. The stippling indicates where the CNN is looking to make a prediction, with the open circles indicating the most important regions.\n",
    "\n",
    "##### Scrolling through the plots, you may see the CNN focusing on areas of:\n",
    "- High IVT\n",
    "- High precipitation\n",
    "- High u-winds\n",
    "- Temperature gradients\n",
    "\n",
    "##### Do you notice any patterns or values where the CNN focuses for prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede6884-60a4-49ca-922e-3c9383ac8506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f52c98e7-21f4-4292-a21d-ee2ab74f3a20",
   "metadata": {},
   "source": [
    "## Downscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11161f2-544f-4ef7-ba86-706d65694087",
   "metadata": {},
   "source": [
    "##### The goal of our CNN is to predict Orographic precipitation gradients... but why? These orographic precipitation gradients can then be used to downscale precipitation over complex terrain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ae34d-e207-4d03-96dd-123c5d5f69cb",
   "metadata": {},
   "source": [
    "##### To make downscaling a quick process, lets depend on the strong correlation between the observed OPG and y-intercepts. This correlation tends to range between 0.6 and 0.95. First, we'll load in the linear relationships between the two variables, the topographic data, and the dates to downscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e837d7e-1a10-43a9-8093-e1b106e7ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "path = \"../datasets/facets_and_opgs/\"\n",
    "\n",
    "# Load in the observed linear relationship between OPG and y-intercepts\n",
    "corrOPGyint  = pd.read_csv(f\"{path}winter_northernUT_lin_model_opg_y-int.csv\", index_col=0)\n",
    "facet_list   = np.asarray(corrOPGyint.columns.values, dtype=float)\n",
    "variables    = corrOPGyint.index.values\n",
    "coefs        = corrOPGyint.values\n",
    "\n",
    "# Load in topographic data needed for downscaling\n",
    "elev            = np.array(pd.read_csv(f\"{path}elevation.csv\", index_col=0))\n",
    "facets          = np.array(pd.read_csv(f\"{path}facet_labels.csv\", index_col=0))\n",
    "lats            = np.array(pd.read_csv(f\"{path}lats.csv\", index_col=0))\n",
    "lons            = np.array(pd.read_csv(f\"{path}lons.csv\", index_col=0))\n",
    "prcp_obs        = np.array(pd.read_csv(f\"{path}winter_northernUT_precip_obs.csv\", index_col=0))\n",
    "prcp_obs_latlon = np.array(pd.read_csv(f\"{path}winter_northernUT_precip_obs_latlon.csv\", index_col=0))\n",
    "\n",
    "# List of dates to plot\n",
    "dates = np.array(['2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10', '2017-01-11', '2017-01-12'], dtype='datetime64')\n",
    "\n",
    "# Find index of these dates within the Xarray\n",
    "idx = IVT.time.dt.date.isin(dates)\n",
    "\n",
    "# Index ERA5 precip variables\n",
    "precip_subset = precip.sel(time=slice(dates[0], dates[-1]))\n",
    "\n",
    "# Subset the observational dates from the mesonet\n",
    "prcp_obs_subset = prcp_obs[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38fcf2-c85f-4bdc-a3da-cd0c71638044",
   "metadata": {},
   "source": [
    "##### Next lets predict OPGs over the same days we have been working with, and convert the standardized values to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c60921-3b8b-470b-bf22-241ef960c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict atmos subset\n",
    "pred_opg = model.predict(atmos_subset)\n",
    "\n",
    "# set zero OPGs to nan\n",
    "pred_opg[pred_opg==0] = np.nan\n",
    "\n",
    "# Convert from standardized OPG to mm/m OPG\n",
    "pred_opg = (pred_opg * opg_sd) + opg_mean\n",
    "\n",
    "print(np.shape(pred_opg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f36c24-b902-4cdc-bb93-548cdc738484",
   "metadata": {},
   "source": [
    "##### Now, we'll use the correlation between OPGs and the y-intercept to formulate the predicted y-intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b9ae5-f8d8-4b35-8bb0-bb20dec08bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array to hold y-intercepts\n",
    "pred_yint = np.zeros(np.shape(pred_opg))\n",
    "\n",
    "# loop through each facet\n",
    "for fi in range(np.shape(pred_opg)[1]):\n",
    "    # Using the fit linear relationship, calculate the y-intercepts\n",
    "    pred_yint[:, fi] = coefs[0, fi]*pred_opg[:, fi] + coefs[1, fi]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b50ac7-dbe0-4d9d-95d7-5d40050c5abe",
   "metadata": {},
   "source": [
    "##### Here, we downscale precipitation to each facet using the predicted OPGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5386163-a91c-4f1f-9bd3-9c29e2b6eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array for the precipitation\n",
    "opg_precip = np.zeros((np.shape(dates)[0], np.shape(elev)[0], np.shape(elev)[1]))\n",
    "\n",
    "# Loop through each facet\n",
    "for fi in range(np.shape(pred_opg)[1]):\n",
    "    # loop through each day\n",
    "    for dayx in range(np.shape(pred_opg)[0]):\n",
    "        # Formulate precipitation total based on elevation of a facet, OPG, and y-intercept\n",
    "        fi_precip = np.squeeze(([facets==facet_list[fi]])*(elev)*(pred_opg[dayx, fi]) + pred_yint[dayx, fi])\n",
    "\n",
    "        # Save precipitation to opg_precip array\n",
    "        opg_precip[dayx, facets==facet_list[fi]] = fi_precip[facets==facet_list[fi]]\n",
    "\n",
    "# Set all negative values to zero, and all non-facets to NaNs\n",
    "opg_precip[opg_precip < 0] = 0\n",
    "opg_precip[:, np.isnan(facets)] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9577ffca-1b35-4489-b35f-be587ff9986c",
   "metadata": {},
   "source": [
    "##### Last, let's plot the ERA5 accumulated precipitation, observed precipitation from mesonet stations, and the CNN downscaled precipitation using OPGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dce0b1-f3b1-4a10-ac3e-3d0f1424e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "extent1 = [-116, -106.5, 38, 43.5] # extent of plotting window in lons then lats\n",
    "extent2 = [-112.5, -111.2, 40.7, 41.6] # extent of plotting window in lons then lats\n",
    "\n",
    "levs    = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]#np.arange(0.1, 50, 5)\n",
    "pr_cmap = ncm.cmapDiscrete('precip3_16lev', indexList=[0, 2, 4, 6, 8, 10, 12, 14, 15, 16])\n",
    "\n",
    "# Define plot type args\n",
    "contourf_args = {'cmap': pr_cmap, 'levels': levs, 'extend': 'max', 'transform': datacrs}\n",
    "scatter_args  = {'s': 50, 'vmin': levs[0], 'vmax': levs[-1], 'cmap': pr_cmap, 'edgecolor': 'black', 'linewidths': 0.5, 'transform': datacrs}\n",
    "plot_args     = {'color': 'black', 'transform': datacrs}\n",
    "\n",
    "\n",
    "for dayx in range(np.shape(pred_opg)[0]):\n",
    "\n",
    "    # remove scatter points that are NaN\n",
    "    idxNaN           = ~np.isnan(prcp_obs_subset[dayx, :])\n",
    "    prcp_obs_subsetx = prcp_obs_subset[dayx, idxNaN]\n",
    "    prcp_obs_latlonx = prcp_obs_latlon[idxNaN, :]\n",
    "\n",
    "    # sort the precipitation data so that the highest observation is ontop\n",
    "    idxSort = np.argsort(prcp_obs_subsetx)\n",
    "    prcp_obs_subsetx = prcp_obs_subsetx[idxSort]\n",
    "    prcp_obs_latlonx = prcp_obs_latlonx[idxSort, :]\n",
    "\n",
    "    ########## PLOT PRECIPITATION FROM ERA5, GHCND, AND CNN DOWNSCALED ##########\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2, 3, figsize = (9, 5), subplot_kw={'projection': projex})\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Plot ERA5\n",
    "    axs[0, 0].contourf(precip_subset.longitude, precip_subset.latitude, precip_subset.precip.values[dayx,:,:], **contourf_args)\n",
    "    scatter = axs[0, 0].scatter(prcp_obs_latlonx[:, 1], prcp_obs_latlonx[:, 0], c=prcp_obs_subsetx, **scatter_args)\n",
    "    axs[0, 0].plot([extent2[0], extent2[1], extent2[1], extent2[0], extent2[0]], \n",
    "                   [extent2[2], extent2[2], extent2[3], extent2[3], extent2[2]], **plot_args)\n",
    "    axs[0, 0].set_title(\"ERA5 Precip.\")\n",
    "    axs[0, 0].set_extent(extent1)\n",
    "    \n",
    "    # Plot Observations\n",
    "    axs[0, 1].scatter(prcp_obs_latlonx[:, 1], prcp_obs_latlonx[:, 0], c=prcp_obs_subsetx, **scatter_args)\n",
    "    axs[0, 1].plot([extent2[0], extent2[1], extent2[1], extent2[0], extent2[0]],\n",
    "                   [extent2[2], extent2[2], extent2[3], extent2[3], extent2[2]], **plot_args)\n",
    "    axs[0, 1].set_title(\"GHCND Precip.\")\n",
    "    axs[0, 1].set_extent(extent1)\n",
    "    \n",
    "    # Plot Downscaled\n",
    "    axs[0, 2].contourf(lons, lats, np.squeeze(opg_precip[dayx,:,:]), **contourf_args)\n",
    "    axs[0, 2].scatter(prcp_obs_latlonx[:, 1], prcp_obs_latlonx[:, 0], c=prcp_obs_subsetx, **scatter_args)\n",
    "    axs[0, 2].plot([extent2[0], extent2[1], extent2[1], extent2[0], extent2[0]],\n",
    "                   [extent2[2], extent2[2], extent2[3], extent2[3], extent2[2]], **plot_args)\n",
    "    axs[0, 2].set_title(\"CNN Downscaled Precip.\")\n",
    "    axs[0, 2].set_extent(extent1)\n",
    "\n",
    "    # Plot ERA5\n",
    "    axs[1, 0].contourf(precip_subset.longitude, precip_subset.latitude, precip_subset.precip.values[dayx,:,:], **contourf_args)\n",
    "    axs[1, 0].scatter(prcp_obs_latlonx[:, 1], prcp_obs_latlonx[:, 0], c=prcp_obs_subsetx, **scatter_args)\n",
    "    axs[1, 0].set_extent(extent2)\n",
    "    \n",
    "    # Plot Observations\n",
    "    axs[1, 1].scatter(prcp_obs_latlonx[:, 1], prcp_obs_latlonx[:, 0], c=prcp_obs_subsetx, **scatter_args)\n",
    "    axs[1, 1].set_extent(extent2)\n",
    "    \n",
    "    # Plot Downscaled\n",
    "    pr = axs[1, 2].contourf(lons, lats, np.squeeze(opg_precip[dayx,:,:]), **contourf_args)\n",
    "    axs[1, 2].scatter(prcp_obs_latlonx[:, 1], prcp_obs_latlonx[:, 0], c=prcp_obs_subsetx, **scatter_args)\n",
    "    axs[1, 2].set_extent(extent2)\n",
    "    \n",
    "    # Add Cartogrphy\n",
    "    for row in range(2):\n",
    "        for col in range(3):\n",
    "            axs[row,col].add_feature(cfeat.LAND, facecolor=[0.2, 0.2, 0.2])\n",
    "            axs[row,col].add_feature(cfeat.RIVERS.with_scale('10m'), edgecolor=\"orangered\")\n",
    "            axs[row,col].add_feature(cfeat.LAKES.with_scale('10m'), edgecolor=\"orangered\", facecolor=[0.2, 0.2, 0.2])\n",
    "            axs[row,col].add_feature(cfeat.STATES.with_scale('10m'), edgecolor=\"orangered\")\n",
    "\n",
    "    # Add legends\n",
    "  ##  legend1 = axs[0, 0].legend(*scatter.legend_elements(),\n",
    "   #                 loc=\"lower left\", title=\"Precip\")\n",
    "   # axs[0, 0].add_artist(legend1)\n",
    "    #axs[0, 0].legend()\n",
    "    axs[0, 0].legend(['GHCND'])\n",
    "    axs[1, 0].legend(['GHCND'])\n",
    "    axs[0, 2].legend(['GHCND'])\n",
    "    axs[1, 2].legend(['GHCND'])\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar_pr = fig.add_axes([0.99, 0.05, 0.03, 0.88])\n",
    "    cb_at = fig.colorbar(pr, cax=cbar_pr, pad=0.0, aspect=15, fraction=0.032)\n",
    "    cb_at.set_label('Precipitation (mm)', size=14)\n",
    "    cb_at.ax.tick_params(labelsize=12)\n",
    "    \n",
    "    # adjust plot\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0)\n",
    "    datex = np.datetime_as_string(dates[dayx], unit=\"D\")\n",
    "    plt.suptitle(datex, y=1.05, weight='bold')\n",
    "    \n",
    "    # Save and show figure\n",
    "    path = \"../figures/\"\n",
    "    plt.savefig(f\"{path}downscaling_{datex}.png\", dpi=200, transparent=True,  bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2a85d-58e6-4d2d-9772-dc11980098bb",
   "metadata": {},
   "source": [
    "##### Above are the comparisons between the ERA5 at 0.5 degree, GHCND observed precipitation, and the precipitation downscaled from the CNN's predicted OPGs. The top row shows the entire northern UT region, and the bottom row zooms into the Salt Lake and Ogden Valley region. \n",
    "\n",
    "##### With the ERA5 output, at a 0.5 degree (~55 km) grid spacing, much of the fine scale variability is gone. This causes the accumulated precipitation to range between 0 and 10 mm for most of the domain, and lacks all terrain variability over complex terrain. \n",
    "\n",
    "##### Downscaling precipitation based on OPGs predicted by a CNN, on a 4 km grid spaced elevation, produces a more realistic distribution of precipitation. However, the more extreme values of precipitation are underestimated. Could this be due to the elevation dataset being too coarse? Or is this the CNN underestimating OPGs?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
